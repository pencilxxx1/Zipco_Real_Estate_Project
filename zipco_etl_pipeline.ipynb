{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7af263c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dotenv in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.9.9)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.32.3)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.0.43)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Necessary Packages\n",
    "!pip install dotenv pandas requests psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a41e7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary librabries needed\n",
    "import psycopg2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, Table, Column, Integer, String, Float, DateTime, MetaData, ForeignKey\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874ee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 500 records for sale.\n",
      "Extracted 500 records for rental.\n",
      "Total records extracted: 1000\n"
     ]
    }
   ],
   "source": [
    "# Extraction Phase: Extract sales and rental data from the Rentcast API\n",
    "\n",
    "# Define API key and URLs\n",
    "API_KEY = \"API_KEY\"  # Replace with your actual API key\n",
    "HEADERS = {\"X-Api-Key\": API_KEY, \"accept\": \"application/json\"}\n",
    "SALE_URL = \"https://api.rentcast.io/v1/listings/sale?city=Austin&state=TX&status=Active&limit=500\"\n",
    "RENTAL_URL = \"https://api.rentcast.io/v1/listings/rental/long-term?city=Austin&state=TX&status=Active&limit=500\"\n",
    "\n",
    "# === Extract Function ===\n",
    "def extract_data(url, category):\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        df[\"listing_category\"] = category  # add category (sale or rental)\n",
    "        print(f\"Extracted {len(df)} records for {category}.\")\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"API request failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Run extractions\n",
    "sales_df = extract_data(SALE_URL, \"sale\")\n",
    "rentals_df = extract_data(RENTAL_URL, \"rental\")\n",
    "\n",
    "# Combine into one dataset\n",
    "combined_df = pd.concat([sales_df, rentals_df], ignore_index=True)\n",
    "print(f\"Total records extracted: {len(combined_df)}\")\n",
    "\n",
    "# Idempotent save to CSV\n",
    "combined_df.to_csv(\"austin_listings.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40b98ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Phase: Clean and transform data\n",
    "\n",
    "# === Transform Function ===\n",
    "def transform_data(sales_df, rentals_df):\n",
    "    \n",
    "    # Handle missing values (based on schema)\n",
    "    combined_df.fillna({\n",
    "        'addressLine2': '', 'county': '', 'lotSize': 0, 'yearBuilt': 0, 'hoa': {'fee': 0},\n",
    "        'listingType': 'Standard', 'daysOnMarket': 0, 'removedDate': None, 'mlsName': '', 'mlsNumber': '',\n",
    "        'listingAgent': {}, 'listingOffice': {}, 'builder': {}, 'history': {}\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Extract nested fields\n",
    "    combined_df['hoa_fee'] = combined_df['hoa'].apply(lambda x: x.get('fee', 0) if isinstance(x, dict) else 0)\n",
    "    combined_df['agent_name'] = combined_df['listingAgent'].apply(lambda x: x.get('name', '') if isinstance(x, dict) else '')\n",
    "    combined_df['agent_phone'] = combined_df['listingAgent'].apply(lambda x: x.get('phone', '') if isinstance(x, dict) else '')\n",
    "    combined_df['agent_email'] = combined_df['listingAgent'].apply(lambda x: x.get('email', '') if isinstance(x, dict) else '')\n",
    "    combined_df['agent_website'] = combined_df['listingAgent'].apply(lambda x: x.get('website', '') if isinstance(x, dict) else '')\n",
    "    combined_df['office_name'] = combined_df['listingOffice'].apply(lambda x: x.get('name', '') if isinstance(x, dict) else '')\n",
    "    combined_df['office_phone'] = combined_df['listingOffice'].apply(lambda x: x.get('phone', '') if isinstance(x, dict) else '')\n",
    "    # Handle builder for 'New Construction'\n",
    "    combined_df['agent_name'] = combined_df.apply(lambda row: row['builder'].get('name', row['agent_name']) if row.get('listingType') == 'New Construction' else row['agent_name'], axis=1)\n",
    "    # Extend similarly for other builder fields if present\n",
    "    \n",
    "    # Convert dates\n",
    "    date_cols = ['listedDate', 'removedDate', 'createdDate', 'lastSeenDate']\n",
    "    for col in date_cols:\n",
    "        if col in combined_df.columns:\n",
    "            combined_df[col] = pd.to_datetime(combined_df[col], errors='coerce')\n",
    "    \n",
    "    # Flatten history object\n",
    "    history_records = []\n",
    "    for idx, row in combined_df.iterrows():\n",
    "        for history_date, event in row.get('history', {}).items():\n",
    "            if isinstance(event, dict):\n",
    "                event['listing_id'] = row['id']\n",
    "                event['history_date'] = history_date\n",
    "                history_records.append(event)\n",
    "    history_df = pd.DataFrame(history_records)\n",
    "    history_df['listedDate'] = pd.to_datetime(history_df['listedDate'], errors='coerce')\n",
    "    history_df['removedDate'] = pd.to_datetime(history_df['removedDate'], errors='coerce')\n",
    "    \n",
    "    return combined_df, history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d55e39d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to real_estate_db database successfully.\n"
     ]
    }
   ],
   "source": [
    "# connect to PostgreSQL database\n",
    "dotenv.load_dotenv() # load .env file\n",
    "DB_USER = os.getenv('DB_USER') # get username\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD') # get password\n",
    "DB_HOST = 'localhost' # get host\n",
    "DB_NAME = 'real_estate_db' # database name\n",
    "DB_PORT = '5432' # default port\n",
    "engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}')\n",
    "\n",
    "print(f\"connected to {DB_NAME} database successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "64d2fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL schema table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Create Schema Function ===\n",
    "def create_postgres_schema(engine):\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS dim_state (\n",
    "                state_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                state VARCHAR(255) UNIQUE NOT NULL\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_city (\n",
    "                city_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                city VARCHAR(255) NOT NULL,\n",
    "                county VARCHAR(255),\n",
    "                state_id INTEGER NOT NULL REFERENCES dim_state(state_id)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_zip (\n",
    "                zip_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                zip_code VARCHAR(255) UNIQUE NOT NULL,\n",
    "                city_id INTEGER NOT NULL REFERENCES dim_city(city_id)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_address (\n",
    "                address_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                address_line1 VARCHAR(255) NOT NULL,\n",
    "                address_line2 VARCHAR(255),\n",
    "                formatted_address VARCHAR(255),\n",
    "                zip_id INTEGER NOT NULL REFERENCES dim_zip(zip_id)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_location (\n",
    "                location_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                latitude FLOAT NOT NULL,\n",
    "                longitude FLOAT NOT NULL,\n",
    "                address_id INTEGER NOT NULL REFERENCES dim_address(address_id)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_hoa (\n",
    "                hoa_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                fee FLOAT\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_property (\n",
    "                property_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                property_type VARCHAR(255) NOT NULL,\n",
    "                bedrooms FLOAT,\n",
    "                bathrooms FLOAT,\n",
    "                square_footage FLOAT,\n",
    "                lot_size FLOAT,\n",
    "                year_built INTEGER,\n",
    "                hoa_id INTEGER REFERENCES dim_hoa(hoa_id)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_date (\n",
    "                date_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                full_date TIMESTAMP NOT NULL,\n",
    "                year INTEGER NOT NULL,\n",
    "                month INTEGER NOT NULL,\n",
    "                day INTEGER NOT NULL\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_office (\n",
    "                office_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                name VARCHAR(255) NOT NULL,\n",
    "                phone VARCHAR(255)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS dim_agent (\n",
    "                agent_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                name VARCHAR(255) NOT NULL,\n",
    "                phone VARCHAR(255),\n",
    "                email VARCHAR(255),\n",
    "                website VARCHAR(255),\n",
    "                type VARCHAR(255) NOT NULL,  -- 'agent' or 'builder'\n",
    "                office_id INTEGER NOT NULL REFERENCES dim_office(office_id)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS fact_listings (\n",
    "                listing_id VARCHAR(255) PRIMARY KEY NOT NULL,\n",
    "                listing_category VARCHAR(255) NOT NULL,  -- 'sale' or 'rental'\n",
    "                price FLOAT NOT NULL,\n",
    "                status VARCHAR(255) NOT NULL,\n",
    "                listing_type VARCHAR(255) NOT NULL,\n",
    "                days_on_market INTEGER,\n",
    "                mls_name VARCHAR(255),\n",
    "                mls_number VARCHAR(255),\n",
    "                created_date TIMESTAMP NOT NULL,\n",
    "                last_seen_date TIMESTAMP,\n",
    "                location_id INTEGER NOT NULL REFERENCES dim_location(location_id),\n",
    "                property_id INTEGER NOT NULL REFERENCES dim_property(property_id),\n",
    "                listed_date_id INTEGER NOT NULL REFERENCES dim_date(date_id),\n",
    "                agent_id INTEGER NOT NULL REFERENCES dim_agent(agent_id)\n",
    "            );\n",
    "\n",
    "            CREATE TABLE IF NOT EXISTS listing_history (\n",
    "                history_id SERIAL PRIMARY KEY NOT NULL,\n",
    "                listing_id VARCHAR(255) NOT NULL REFERENCES fact_listings(listing_id),\n",
    "                history_date VARCHAR(255) NOT NULL,\n",
    "                event VARCHAR(255) NOT NULL,\n",
    "                price FLOAT,\n",
    "                listing_type VARCHAR(255),\n",
    "                days_on_market INTEGER,\n",
    "                listed_date TIMESTAMP,\n",
    "                removed_date TIMESTAMP\n",
    "            );\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "        \n",
    "print(\"PostgreSQL schema table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cb33367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake schema created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create snowflake schema in PostgreSQL (using SQLAlchemy)\n",
    "from sqlalchemy import (\n",
    "    Table, Column, Integer, String, Float, DateTime, MetaData, ForeignKey\n",
    ")\n",
    "\n",
    "def create_snowflake_schema(engine):\n",
    "    metadata = MetaData()\n",
    "    \n",
    "    dim_state = Table('dim_state', metadata,\n",
    "        Column('state_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('state', String(255), unique=True, nullable=False)\n",
    "    )\n",
    "\n",
    "    dim_city = Table('dim_city', metadata,\n",
    "        Column('city_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('city', String(255), nullable=False),\n",
    "        Column('county', String(255)),\n",
    "        Column('state_id', Integer, ForeignKey('dim_state.state_id'), nullable=False)\n",
    "    )\n",
    "\n",
    "    dim_zip = Table('dim_zip', metadata,\n",
    "        Column('zip_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('zip_code', String(255), unique=True, nullable=False),\n",
    "        Column('city_id', Integer, ForeignKey('dim_city.city_id'), nullable=False)\n",
    "    )\n",
    "\n",
    "    dim_address = Table('dim_address', metadata,\n",
    "        Column('address_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('address_line1', String(255), nullable=False),\n",
    "        Column('address_line2', String(255)),\n",
    "        Column('formatted_address', String(255)),\n",
    "        Column('zip_id', Integer, ForeignKey('dim_zip.zip_id'), nullable=False)\n",
    "    )\n",
    "\n",
    "    dim_location = Table('dim_location', metadata,\n",
    "        Column('location_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('latitude', Float, nullable=False),\n",
    "        Column('longitude', Float, nullable=False),\n",
    "        Column('address_id', Integer, ForeignKey('dim_address.address_id'), nullable=False)\n",
    "    )\n",
    "\n",
    "    dim_hoa = Table('dim_hoa', metadata,\n",
    "        Column('hoa_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('fee', Float)\n",
    "    )\n",
    "    \n",
    "    dim_property = Table('dim_property', metadata,\n",
    "        Column('property_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('property_type', String(255), nullable=False),\n",
    "        Column('bedrooms', Float),\n",
    "        Column('bathrooms', Float),\n",
    "        Column('square_footage', Float),\n",
    "        Column('lot_size', Float),\n",
    "        Column('year_built', Integer),\n",
    "        Column('hoa_id', Integer, ForeignKey('dim_hoa.hoa_id'))\n",
    "    )\n",
    "\n",
    "    dim_date = Table('dim_date', metadata,\n",
    "        Column('date_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('full_date', DateTime, nullable=False),\n",
    "        Column('year', Integer, nullable=False),\n",
    "        Column('month', Integer, nullable=False),\n",
    "        Column('day', Integer, nullable=False)\n",
    "    )\n",
    "\n",
    "    dim_office = Table('dim_office', metadata,\n",
    "        Column('office_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('name', String(255), nullable=False),\n",
    "        Column('phone', String(255))\n",
    "    )\n",
    "    \n",
    "    dim_agent = Table('dim_agent', metadata,\n",
    "        Column('agent_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('name', String(255), nullable=False),\n",
    "        Column('phone', String(255)),\n",
    "        Column('email', String(255)),\n",
    "        Column('website', String(255)),\n",
    "        Column('type', String(255), nullable=False),  # 'agent' or 'builder'\n",
    "        Column('office_id', Integer, ForeignKey('dim_office.office_id'), nullable=False)\n",
    "    )\n",
    "\n",
    "    fact_listings = Table('fact_listings', metadata,\n",
    "        Column('listing_id', String(255), primary_key=True, nullable=False),\n",
    "        Column('listing_category', String(255), nullable=False),  # 'sale' or 'rental'\n",
    "        Column('price', Float, nullable=False),\n",
    "        Column('status', String(255), nullable=False),\n",
    "        Column('listing_type', String(255), nullable=False),\n",
    "        Column('days_on_market', Integer),\n",
    "        Column('mls_name', String(255)),\n",
    "        Column('mls_number', String(255)),\n",
    "        Column('created_date', DateTime, nullable=False),\n",
    "        Column('last_seen_date', DateTime),\n",
    "        Column('location_id', Integer, ForeignKey('dim_location.location_id'), nullable=False),\n",
    "        Column('property_id', Integer, ForeignKey('dim_property.property_id'), nullable=False),\n",
    "        Column('listed_date_id', Integer, ForeignKey('dim_date.date_id'), nullable=False),\n",
    "        Column('agent_id', Integer, ForeignKey('dim_agent.agent_id'), nullable=False)\n",
    "    )\n",
    "\n",
    "    listing_history = Table('listing_history', metadata,\n",
    "        Column('history_id', Integer, primary_key=True, autoincrement=True, nullable=False),\n",
    "        Column('listing_id', String(255), ForeignKey('fact_listings.listing_id'), nullable=False),\n",
    "        Column('history_date', String(255), nullable=False),\n",
    "        Column('event', String(255), nullable=False),\n",
    "        Column('price', Float),\n",
    "        Column('listing_type', String(255)),\n",
    "        Column('days_on_market', Integer),\n",
    "        Column('listed_date', DateTime),\n",
    "        Column('removed_date', DateTime)\n",
    "    )\n",
    "    \n",
    "    metadata.create_all(engine)\n",
    "    \n",
    "print(\"Snowflake schema created successfully.\")\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f64bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded into real_estate_db successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# === API Setup ===\n",
    "API_KEY = 'API_KEY_PLACEHOLDER'  # Replace with your actual API key\n",
    "HEADERS = {\"X-Api-Key\": API_KEY, \"accept\": \"application/json\"}\n",
    "SALE_URL = \"https://api.rentcast.io/v1/listings/sale?city=Austin&state=TX&status=Active&limit=500\"\n",
    "RENTAL_URL = \"https://api.rentcast.io/v1/listings/rental/long-term?city=Austin&state=TX&status=Active&limit=500\"\n",
    "\n",
    "\n",
    "# === Database Setup ===\n",
    "dotenv.load_dotenv()\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = 'localhost'\n",
    "DB_NAME = 'real_estate_db'\n",
    "DB_PORT = '5432'\n",
    "\n",
    "# Create SQLAlchemy engine for pandas to_sql\n",
    "ENGINE = create_engine(\n",
    "    f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "def load_data(combined_df, history_df):\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Always insert into history (via SQLAlchemy)\n",
    "    history_df.to_sql('listing_history', ENGINE, if_exists='append', index=False)\n",
    "    \n",
    "    \n",
    "    # Insert into normalized dimensions (example logic; use upserts and retrieve IDs)\n",
    "    for _, row in combined_df.iterrows():\n",
    "        # State\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_state (state) VALUES (%s)\n",
    "            ON CONFLICT (state) DO UPDATE SET state = EXCLUDED.state\n",
    "            RETURNING state_id\n",
    "        \"\"\", (row['state'],))\n",
    "        state_id = cur.fetchone()[0]\n",
    "        \n",
    "        # City\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_city (city, county, state_id) VALUES (%s, %s, %s)\n",
    "            ON CONFLICT (city, state_id) DO UPDATE SET county = EXCLUDED.county\n",
    "            RETURNING city_id\n",
    "        \"\"\", (row['city'], row['county'], state_id))\n",
    "        city_id = cur.fetchone()[0]\n",
    "        \n",
    "        # Zip\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_zip (zip_code, city_id) VALUES (%s, %s)\n",
    "            ON CONFLICT (zip_code) DO UPDATE SET city_id = EXCLUDED.city_id\n",
    "            RETURNING zip_id\n",
    "        \"\"\", (row['zipCode'], city_id))\n",
    "        zip_id = cur.fetchone()[0]\n",
    "        \n",
    "        # Address\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_address (address_line1, address_line2, formatted_address, zip_id)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ON CONFLICT (formatted_address) DO NOTHING\n",
    "            RETURNING address_id\n",
    "        \"\"\", (row['addressLine1'], row['addressLine2'], row['formattedAddress'], zip_id))\n",
    "        address_id = cur.fetchone()[0] if cur.rowcount > 0 else None  # Query if exists\n",
    "        \n",
    "        # Location\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_location (latitude, longitude, address_id) VALUES (%s, %s, %s)\n",
    "            ON CONFLICT (latitude, longitude) DO NOTHING\n",
    "            RETURNING location_id\n",
    "        \"\"\", (row['latitude'], row['longitude'], address_id))\n",
    "        location_id = cur.fetchone()[0] if cur.rowcount > 0 else None\n",
    "        \n",
    "        # HOA\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_hoa (fee) VALUES (%s)\n",
    "            ON CONFLICT (fee) DO NOTHING\n",
    "            RETURNING hoa_id\n",
    "        \"\"\", (row['hoa_fee'],))\n",
    "        hoa_id = cur.fetchone()[0] if cur.rowcount > 0 else None\n",
    "        \n",
    "        # Property\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_property (property_type, bedrooms, bathrooms, square_footage, lot_size, year_built, hoa_id)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "            RETURNING property_id\n",
    "        \"\"\", (row['propertyType'], row['bedrooms'], row['bathrooms'], row['squareFootage'], row['lotSize'], row['yearBuilt'], hoa_id))\n",
    "        property_id = cur.fetchone()[0]\n",
    "        \n",
    "        # Date (for listedDate)\n",
    "        listed_date = row.get('listedDate')\n",
    "        if listed_date:\n",
    "            year, month, day = listed_date.year, listed_date.month, listed_date.day\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO dim_date (full_date, year, month, day) VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (full_date) DO NOTHING\n",
    "                RETURNING date_id\n",
    "            \"\"\", (listed_date, year, month, day))\n",
    "            date_id = cur.fetchone()[0] if cur.rowcount > 0 else None\n",
    "        \n",
    "        # Office\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_office (name, phone) VALUES (%s, %s)\n",
    "            ON CONFLICT (name) DO UPDATE SET phone = EXCLUDED.phone\n",
    "            RETURNING office_id\n",
    "        \"\"\", (row['office_name'], row['office_phone']))\n",
    "        office_id = cur.fetchone()[0]\n",
    "        \n",
    "        # Agent\n",
    "        agent_type = 'builder' if row.get('listingType') == 'New Construction' else 'agent'\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO dim_agent (name, phone, email, website, type, office_id)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "            RETURNING agent_id\n",
    "        \"\"\", (row['agent_name'], row['agent_phone'], row['agent_email'], row['agent_website'], agent_type, office_id))\n",
    "        agent_id = cur.fetchone()[0]\n",
    "        \n",
    "        # Fact Listings (upsert)\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO fact_listings (listing_id, listing_category, price, status, listing_type, days_on_market,\n",
    "                                       mls_name, mls_number, created_date, last_seen_date, location_id, property_id,\n",
    "                                       listed_date_id, agent_id)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (listing_id) DO UPDATE SET price = EXCLUDED.price, status = EXCLUDED.status  -- Update key fields\n",
    "        \"\"\", (row['id'], row['listing_category'], row['price'], row['status'], row.get('listingType'), row.get('daysOnMarket'),\n",
    "              row.get('mlsName'), row.get('mlsNumber'), row.get('createdDate'), row.get('lastSeenDate'), location_id,\n",
    "              property_id, date_id, agent_id))\n",
    "        \n",
    "        # History\n",
    "        for _, hrow in history_df[history_df['listing_id'] == row['id']].iterrows():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO listing_history (listing_id, history_date, event, price, listing_type, days_on_market,\n",
    "                                             listed_date, removed_date)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\", (hrow['listing_id'], hrow['history_date'], hrow.get('event'), hrow.get('price'), hrow.get('listingType'),\n",
    "                  hrow.get('daysOnMarket'), hrow.get('listedDate'), hrow.get('removedDate')))\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "print(\"✅ Data loaded into {} successfully.\".format(DB_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67182f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Run complete ETL pipeline\n",
    "# Extract\n",
    "def extract_data(url, category):\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            df = pd.DataFrame([data])   # wrap dict in list\n",
    "        else:\n",
    "            df = pd.DataFrame(data)     # already list of dicts\n",
    "\n",
    "        if not df.empty:\n",
    "            df[\"listing_category\"] = category\n",
    "            df[\"extraction_timestamp\"] = datetime.utcnow()\n",
    "\n",
    "        print(f\"Extracted {len(df)} records for {category}\")\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sales_df = extract_data(SALE_URL, \"sale\")\n",
    "    rentals_df = extract_data(RENTAL_URL, \"rental\")\n",
    "    \n",
    "# Transform\n",
    "def transform_data(sales_df, rentals_df):\n",
    "    # Combine dataframes\n",
    "    combined_df = pd.concat([sales_df, rentals_df], ignore_index=True)\n",
    "    combined_df, history_df = transform_data(sales_df, rentals_df)\n",
    "    \n",
    "    # Create schema\n",
    "    create_postgres_schema(engine) # Create Postgres schema if not exists\n",
    "    create_snowflake_schema(engine) # Create snowflake schema if not exists\n",
    "    \n",
    "\n",
    "# Load\n",
    "def load_data(combined_df, history_df):\n",
    "    \n",
    "    conn = psycopg2.connect(host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    \n",
    "print(\"ETL Pipeline completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install apache-airflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811b9ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# pip install apache-airflow\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproviders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpostgres\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpostgres\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PostgresHook\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproviders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowflake\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SnowflakeHook\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.models import Variable\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Reusable Functions ---\n",
    "def extract_incremental(table_name, last_updated_column, **context):\n",
    "    last_run = Variable.get(f\"{table_name}_last_run\", default_var=\"2000-01-01 00:00:00\")\n",
    "    pg_hook = PostgresHook(postgres_conn_id=\"postgres_conn\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table_name}\n",
    "        WHERE {last_updated_column} > '{last_run}'\n",
    "        ORDER BY {last_updated_column} ASC;\n",
    "    \"\"\"\n",
    "    records = pg_hook.get_records(sql)\n",
    "    if records:\n",
    "        cursor = pg_hook.get_cursor()\n",
    "        idx = [desc[0] for desc in cursor.description].index(last_updated_column)\n",
    "        max_date = max(row[idx] for row in records)\n",
    "        context['ti'].xcom_push(key=\"max_updated_date\", value=str(max_date))\n",
    "    return records\n",
    "\n",
    "def merge_incremental(table_name, records, **context):\n",
    "    if not records:\n",
    "        return\n",
    "    sf_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_conn\")\n",
    "    col_count = len(records[0])\n",
    "    placeholders = \", \".join([\"%s\"] * col_count)\n",
    "    pk_map = {\n",
    "        \"dim_state\": \"state_id\",\n",
    "        \"dim_city\": \"city_id\",\n",
    "        \"dim_zip\": \"zip_id\",\n",
    "        \"dim_address\": \"address_id\",\n",
    "        \"dim_location\": \"location_id\",\n",
    "        \"dim_hoa\": \"hoa_id\",\n",
    "        \"dim_property\": \"property_id\",\n",
    "        \"dim_date\": \"date_id\",\n",
    "        \"dim_office\": \"office_id\",\n",
    "        \"dim_agent\": \"agent_id\",\n",
    "        \"fact_listings\": \"listing_id\"\n",
    "    }\n",
    "    primary_key = pk_map.get(table_name, \"id\")\n",
    "    for row in records:\n",
    "        columns = [f\"col{i}\" for i in range(col_count)]\n",
    "        update_set = \", \".join([f\"{col}=source.{col}\" for col in columns[1:]])\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO {table_name} AS target\n",
    "            USING (SELECT {placeholders}) AS source ({', '.join(columns)})\n",
    "            ON target.{primary_key} = source.{columns[0]}\n",
    "            WHEN MATCHED THEN UPDATE SET {update_set}\n",
    "            WHEN NOT MATCHED THEN INSERT ({', '.join(columns)}) VALUES ({placeholders});\n",
    "        \"\"\"\n",
    "        sf_hook.run(merge_sql, parameters=row*2)\n",
    "    max_date = context['ti'].xcom_pull(task_ids=f\"extract_{table_name}\", key=\"max_updated_date\")\n",
    "    if max_date:\n",
    "        Variable.set(f\"{table_name}_last_run\", max_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ba247",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION_TABLES = [\n",
    "    (\"dim_state\", \"state_id\"),\n",
    "    (\"dim_city\", \"city_id\"),\n",
    "    (\"dim_zip\", \"zip_id\"),\n",
    "    (\"dim_address\", \"address_id\"),\n",
    "    (\"dim_location\", \"location_id\"),\n",
    "    (\"dim_hoa\", \"hoa_id\"),\n",
    "    (\"dim_property\", \"property_id\"),\n",
    "    (\"dim_date\", \"date_id\"),\n",
    "    (\"dim_office\", \"office_id\"),\n",
    "    (\"dim_agent\", \"agent_id\")\n",
    "]\n",
    "\n",
    "FACT_TABLE = [(\"fact_listings\", \"created_date\")]\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"parallel_incremental_etl\",\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False,\n",
    "    tags=[\"etl\", \"incremental\", \"merge\", \"parallel\"]\n",
    ") as dag:\n",
    "\n",
    "    # --- Create extract & merge tasks for dimensions ---\n",
    "    dimension_merge_tasks = []\n",
    "\n",
    "    for table_name, last_updated_col in DIMENSION_TABLES:\n",
    "        extract_task = PythonOperator(\n",
    "            task_id=f\"extract_{table_name}\",\n",
    "            python_callable=extract_incremental,\n",
    "            op_kwargs={\"table_name\": table_name, \"last_updated_column\": last_updated_col},\n",
    "            provide_context=True\n",
    "        )\n",
    "\n",
    "        merge_task = PythonOperator(\n",
    "            task_id=f\"merge_{table_name}\",\n",
    "            python_callable=merge_incremental,\n",
    "            op_kwargs={\"table_name\": table_name},\n",
    "            provide_context=True\n",
    "        )\n",
    "\n",
    "        extract_task >> merge_task\n",
    "        dimension_merge_tasks.append(merge_task)\n",
    "\n",
    "    # --- Fact table tasks ---\n",
    "    for table_name, last_updated_col in FACT_TABLE:\n",
    "        extract_task = PythonOperator(\n",
    "            task_id=f\"extract_{table_name}\",\n",
    "            python_callable=extract_incremental,\n",
    "            op_kwargs={\"table_name\": table_name, \"last_updated_column\": last_updated_col},\n",
    "            provide_context=True\n",
    "        )\n",
    "\n",
    "        merge_task = PythonOperator(\n",
    "            task_id=f\"merge_{table_name}\",\n",
    "            python_callable=merge_incremental,\n",
    "            op_kwargs={\"table_name\": table_name},\n",
    "            provide_context=True\n",
    "        )\n",
    "\n",
    "        # Fact table depends on all dimension merges\n",
    "        dimension_merge_tasks >> extract_task\n",
    "        extract_task >> merge_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
